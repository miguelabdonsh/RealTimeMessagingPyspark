{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Crear un SparkContext con un nombre de aplicación\n",
    "sc = SparkContext(appName=\"MessageStreamingApp\")\n",
    "\n",
    "# Crear un StreamingContext con un intervalo de batch de 1 segundo\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Crear un DStream para recibir los datos del servidor\n",
    "lines = ssc.socketTextStream(\"localhost\", 12345)  # reemplazar localhost y 9999 con la dirección IP y el puerto del servidor\n",
    "\n",
    "# Procesar los datos\n",
    "\"\"\"\n",
    "En este ejemplo, se aplica un procesamiento básico a los datos. \n",
    "Primero, se divide cada línea en palabras utilizando flatMap. \n",
    "Luego, se asigna un valor de 1 a cada palabra mediante map. \n",
    "A continuación, se agrupan las palabras y se cuentan las ocurrencias mediante reduceByKey. \n",
    "Finalmente, se imprime el resultado utilizando pprint (pretty print) para mostrar los recuentos de palabras.\n",
    "\"\"\"\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "word_counts.pprint()\n",
    "\n",
    "# Comenzar el procesamiento de streaming\n",
    "ssc.start()\n",
    "\n",
    "# Esperar a que se complete el procesamiento\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "###VADER Y KEYWORDS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType, BooleanType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Crear el SparkSession y StreamingContext\n",
    "spark = SparkSession.builder.appName(\"NetworkWordCount\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Ajustar el nivel de registro a ERROR para evitar advertencias\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Crear un DStream que se conectará al hostname:port, por ejemplo localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 12345)\n",
    "\n",
    "# Crear el analizador de sentimientos Vader\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Lista de palabras clave que podrían indicar que un niño está pidiendo ayuda\n",
    "keywords = [\"help\", \"need\", \"trouble\", \"danger\", \"unsafe\", \"scared\", \"fear\", \"worried\", \"hurt\", \"abuse\"]\n",
    "\n",
    "# Función que busca coincidencias con las keywords.\n",
    "def check_keywords(message):\n",
    "    # Convertir el mensaje a minúsculas\n",
    "    message = message.lower()\n",
    "    \n",
    "    # Comprobar si alguna palabra clave está en el mensaje\n",
    "    for keyword in keywords:\n",
    "        if keyword in message:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Definir la UDF a partir de la función anterior para poder aplicarla a un DataFrame.\n",
    "check_keywords_udf = udf(check_keywords, BooleanType())\n",
    "\n",
    "# Definir una función para calcular la puntuación de sentimiento de un mensaje\n",
    "def get_sentiment_score(message):\n",
    "    score = analyser.polarity_scores(message)\n",
    "    return score['compound']\n",
    "\n",
    "# Definir otra UDF\n",
    "get_sentiment_score_udf = udf(get_sentiment_score, FloatType())\n",
    "\n",
    "# Función para procesar los datos recibidos (procesar cada RDD)\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    print(\"RDD count: \", rdd.count())  # Imprimir el conteo del RDD\n",
    "    \n",
    "    try:\n",
    "        # Crear un DataFrame a partir del RDD\n",
    "        df = spark.createDataFrame(rdd, \"string\").toDF(\"message\")\n",
    "        \n",
    "        # Verificar si hay al menos una fila con ese mensaje para parar el contexto de streaming si lo hay\n",
    "        if df.filter(df.message == \"SCRAPING_DONE\").count() > 0:\n",
    "            ssc.stop()\n",
    "        \n",
    "        # Crear una nueva columna \"is_help\" que será True si el mensaje está pidiendo ayuda y False en caso contrario\n",
    "        df = df.withColumn(\"is_help\", check_keywords_udf(df[\"message\"]))\n",
    "        \n",
    "        # Crear una nueva columna \"sentiment_score\" que será la puntuación de sentimiento del mensaje\n",
    "        df = df.withColumn(\"sentiment_score\", get_sentiment_score_udf(df[\"message\"]))\n",
    "        \n",
    "        df.show()\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        \n",
    "\n",
    "# Procesar cada RDD en el DStream\n",
    "lines.foreachRDD(process)\n",
    "\n",
    "# Iniciar el streaming\n",
    "ssc.start()\n",
    "\n",
    "# Esperar a que se complete el procesamiento de los datos\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NLP NO SUPERVISADO\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Crear el SparkSession y StreamingContext\n",
    "spark = SparkSession.builder.appName(\"NetworkWordCount\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Ajustar el nivel de registro a ERROR para evitar advertencias\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Crear un DStream que se conectará al hostname:port para recibir los datos\n",
    "lines = ssc.socketTextStream(\"localhost\", 12345)\n",
    "\n",
    "# Crear el analizador de sentimientos\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Definir una función para calcular la puntuación de sentimiento de un mensaje\n",
    "def get_sentiment_score(message):\n",
    "    score = analyser.polarity_scores(message)\n",
    "    return score['compound']\n",
    "\n",
    "# Crear una función UDF para usar en el DataFrame\n",
    "get_sentiment_score_udf = udf(get_sentiment_score, FloatType())\n",
    "\n",
    "# Definir una función para procesar los datos recibidos (cada RDD)\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "\n",
    "    try:\n",
    "        # Crear un DataFrame a partir del RDD\n",
    "        df = spark.createDataFrame(rdd, \"string\").toDF(\"message\")\n",
    "\n",
    "        # Verificar si hay al menos una fila con ese mensaje para parar el contexto de streaming si lo hay\n",
    "        if df.filter(df.message == \"SCRAPING_DONE\").count() > 0:\n",
    "            ssc.stop()\n",
    "\n",
    "        # Crear una nueva columna \"sentiment_score\" que será la puntuación de sentimiento del mensaje\n",
    "        df = df.withColumn(\"sentiment_score\", get_sentiment_score_udf(df[\"message\"]))\n",
    "\n",
    "        # Aplicar NLP no supervisado creando instancias con Tokenizer, StopWordsRemover, CountVectorizer y LDA\n",
    "        tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "        remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "        cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "        lda = LDA(k=2, maxIter=10, featuresCol=\"features\")  # 2 tópicos\n",
    "\n",
    "        # Crear un pipeline que encapsula estas etapas del procesamiento.\n",
    "        pipeline = Pipeline(stages=[tokenizer, remover, cv, lda])\n",
    "        model = pipeline.fit(df)\n",
    "        result = model.transform(df)\n",
    "\n",
    "        result.show()\n",
    "\n",
    "        # Guardar los resultados en un archivo CSV\n",
    "        #result.select(\"sentiment_score\").coalesce(1).write.csv(\"apartadoC.csv\", header=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "# Procesar cada RDD en el DStream\n",
    "lines.foreachRDD(process)\n",
    "\n",
    "# Iniciar el streaming\n",
    "ssc.start()\n",
    "\n",
    "# Esperar a que se complete el procesamiento de los datos\n",
    "ssc.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
